---
title: Research
page-layout: article
---


## Working papers

- **Dake Bu**, Wei Huang, Andi Han, Atsushi Nitanda, Qingfu Zhang, Hau-San Wong, and Taiji Suzuki. Provable Benefit of
Curriculum in Transformer Tree-Reasoning Post-Training.

- **Dake Bu**, Wei Huang, Andi Han, Atsushi Nitanda, Bo Xue, Qingfu Zhang, Hau-San Wong, and Taiji Suzuki. Consistency Is Not Always
Correct: Towards Understanding the Role of Exploration in Post-Training Reasoning.

## Publications

- **Dake Bu**, Wei Huang, Andi Han, Atsushi Nitanda, Qingfu Zhang, Hau-San Wong, and Taiji Suzuki. Provable In-Context Vector
 Arithmetic via Retrieving Task Concepts. The 42nd International Conference on Machine Learning (ICML2025).

- Bo Xue, **Dake Bu**, Ji Cheng, Yuanyu Wan, Qingfu Zhang. Multi-objective Linear Reinforcement Learning with Lexicographic
 Rewards. The 42nd International Conference on Machine Learning (ICML2025).

- **Dake Bu**, Wei Huang, Andi Han, Atsushi Nitanda, Taiji Suzuki, Qingfu Zhang, Hau-San Wong: Provably Transformers Harness
 Multi-Concept Word Semantics for Efficient In-Context Learning. Advances in Neural Information Processing Systems 37 (NeurIPS
 2024).

- **Dake Bu**, Wei Huang, Taiji Suzuki, Ji Cheng, Qingfu Zhang, Zhiqiang Xu, Hau-San Wong: Provably Neural Active Learning
SucceedsviaPrioritizingPerplexingSamples (ICML2024).
