[
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "CS3483 (2025) â€” Multi-model Interface Design."
  },
  {
    "objectID": "teaching/index.html#current-courses",
    "href": "teaching/index.html#current-courses",
    "title": "Teaching",
    "section": "",
    "text": "CS3483 (2025) â€” Multi-model Interface Design."
  },
  {
    "objectID": "teaching/index.html#past-courses",
    "href": "teaching/index.html#past-courses",
    "title": "Teaching",
    "section": "Past courses",
    "text": "Past courses\n\nCS3481 (2024) â€” Fundamentals of Data Science.\nCS3483 (2023) â€” Multi-model Interface Design."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dake Bu",
    "section": "",
    "text": "Dake Bu\n\n\n\n\n\nPhd Candidate, Department of Computer Science, City University of Hong Kong.\nHi! I am currently a PhD candidate at Optima Group, Department of Computer Science, City University of Hong Kong, advised by Prof.Â Hau-San Wong and Prof.Â Qingfu Zhang. Iâ€™m also fortunate to work with Prof.Â Wei Huang and Prof.Â Andi Han. Before that, I completed my bachelor degree in Mathematics at Xiâ€™an Jiaotong University, advised by Prof.Â Hui Li.\nI am currently an incoming 1-year intern to CFAR, A*STAR, where I will be supervised by Prof.Â Atsushi Nitanda. Before that, I spent one year at the University of Tokyo as a member of the Deep Learning Theory Team, RIKEN AIP, advised by Prof.Â Taiji Suzuki.\nMy research broadly covers theoretical foundations of deep learning, distribution optimization, mean-field optimization, sampling and reinforcement learning.\nYou can find a full list of my papers on Google Scholar."
  },
  {
    "objectID": "index.html#news",
    "href": "index.html#news",
    "title": "Dake Bu",
    "section": "News",
    "text": "News\n\n2025-01 â€” Two papers on theoretical foundation of task vector in In-Context Learning, and Multi-objective Reinforcement Learning with Lexicographic Rewards are accepted to ICML 2025.\n\n2024-09 â€” One paper on In context learning with multi-concept word semantics is accepted to NeurIPS 2024.\n\n2024-01 â€” One paper on theoretical foundation of neural active learning accepted to ICML 2025."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Dake Bu",
    "section": "Education",
    "text": "Education\n\nOct.Â 2023 â€“ Present â€” PhD Candidate, Department of Computer Science, City University of Hong Kong (CityUHK).\n\nAug.Â 2019 â€“ Jun.Â 2023 â€” School of Mathematics and Statistics, Xiâ€™an Jiaotong University (XJTU)."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Dake Bu",
    "section": "Contact",
    "text": "Contact\n\nAffiliation: Department of Computer Science, City University of Hong Kong\n\nLocation: Singapore\n\nEmail: &lt;dakebu2-c[at]my.cityu[dot]edu[dot]hk&gt;"
  },
  {
    "objectID": "index.html#links",
    "href": "index.html#links",
    "title": "Dake Bu",
    "section": "Links",
    "text": "Links\n\nðŸ“„ Curriculum Vitae\n\nðŸ“š Google Scholar"
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "I keep my CV as a PDF.\n\nDownload CV (PDF)\n\nPut your cv.pdf file into this folder: cv/cv.pdf."
  },
  {
    "objectID": "posts/hello-world.html",
    "href": "posts/hello-world.html",
    "title": "Hello, Quarto",
    "section": "",
    "text": "This is a placeholder post. Replace it with your own content, or delete the posts/ folder if you donâ€™t want a blog."
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "Dake Bu, Wei Huang, Andi Han, Atsushi Nitanda, Qingfu Zhang, Hau-San Wong, and Taiji Suzuki. Provable Benefit of Curriculum in Transformer Tree-Reasoning Post-Training.\nDake Bu, Wei Huang, Andi Han, Atsushi Nitanda, Bo Xue, Qingfu Zhang, Hau-San Wong, and Taiji Suzuki. Consistency Is Not Always Correct: Towards Understanding the Role of Exploration in Post-Training Reasoning."
  },
  {
    "objectID": "research/index.html#working-papers",
    "href": "research/index.html#working-papers",
    "title": "Research",
    "section": "",
    "text": "Dake Bu, Wei Huang, Andi Han, Atsushi Nitanda, Qingfu Zhang, Hau-San Wong, and Taiji Suzuki. Provable Benefit of Curriculum in Transformer Tree-Reasoning Post-Training.\nDake Bu, Wei Huang, Andi Han, Atsushi Nitanda, Bo Xue, Qingfu Zhang, Hau-San Wong, and Taiji Suzuki. Consistency Is Not Always Correct: Towards Understanding the Role of Exploration in Post-Training Reasoning."
  },
  {
    "objectID": "research/index.html#publications",
    "href": "research/index.html#publications",
    "title": "Research",
    "section": "Publications",
    "text": "Publications\n\nDake Bu, Wei Huang, Andi Han, Atsushi Nitanda, Qingfu Zhang, Hau-San Wong, and Taiji Suzuki. Provable In-Context Vector Arithmetic via Retrieving Task Concepts. The 42nd International Conference on Machine Learning (ICML2025).\nBo Xue, Dake Bu, Ji Cheng, Yuanyu Wan, Qingfu Zhang. Multi-objective Linear Reinforcement Learning with Lexicographic Rewards. The 42nd International Conference on Machine Learning (ICML2025).\nDake Bu, Wei Huang, Andi Han, Atsushi Nitanda, Taiji Suzuki, Qingfu Zhang, Hau-San Wong: Provably Transformers Harness Multi-Concept Word Semantics for Efficient In-Context Learning. Advances in Neural Information Processing Systems 37 (NeurIPS 2024).\nDake Bu, Wei Huang, Taiji Suzuki, Ji Cheng, Qingfu Zhang, Zhiqiang Xu, Hau-San Wong: Provably Neural Active Learning SucceedsviaPrioritizingPerplexingSamples (ICML2024)."
  }
]